{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline\n",
    "\n",
    "This model illustrates how to use the DeBERTa-v3-base-mnli-fever-anli model to perform specialized inference on the ANLI dataset.\n",
    "This dataset has 184M parameters. It was trained in 2021 on the basis of a BERT-like embedding approach: \n",
    "* The premise and the hypothesis are encoded using the DeBERTa-v3-base contextual encoder\n",
    "* The encodings are then compared on a fine-tuned model to predict a distribution over the classification labels (entailment, contradiction, neutral)\n",
    "\n",
    "Reported accuracy on ANLI is 0.495 (see https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a47aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment': 6.6, 'neutral': 17.3, 'contradiction': 76.1}\n"
     ]
    }
   ],
   "source": [
    "premise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\n",
    "hypothesis = \"The movie was good.\"\n",
    "\n",
    "input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfe31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(premise, hypothesis):\n",
    "    input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(input[\"input_ids\"].to(device))\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2954d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0.1, 'neutral': 99.8, 'contradiction': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"The weather is nice today.\", \"It is sunny outside.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923ea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(pred_dict):\n",
    "    if pred_dict[\"entailment\"] > pred_dict[\"contradiction\"]  and pred_dict[\"entailment\"] > pred_dict[\"neutral\"]:\n",
    "        return \"entailment\"\n",
    "    elif pred_dict[\"contradiction\"] > pred_dict[\"entailment\"]  and pred_dict[\"contradiction\"] > pred_dict[\"neutral\"]:\n",
    "        return \"contradiction\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af257dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"The weather is nice today.\", \"It is sunny outside.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "929632f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entailment'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is nice today.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "747c0cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contradiction'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is terrible today.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8262068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the ANLI dataset\n",
    "from tqdm import tqdm\n",
    "def evaluate_on_dataset(dataset):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    for example in tqdm(dataset):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = evaluate(premise, hypothesis)\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'prediction': prediction,\n",
    "            'pred_label': get_prediction(prediction),\n",
    "            'gold_label': label_names[example['label']],\n",
    "            'reason': example['reason']\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf2bd4",
   "metadata": {},
   "source": [
    "### Task 1.1 - Evaluating ANLI samples on test sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50adfd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [16:45<00:00,  1.01s/it] \n",
      "100%|██████████| 1000/1000 [17:04<00:00,  1.02s/it]\n",
      "100%|██████████| 1200/1200 [27:07<00:00,  1.36s/it]   \n"
     ]
    }
   ],
   "source": [
    "pred_test_r1 = evaluate_on_dataset(dataset['test_r1'])\n",
    "pred_test_r2 = evaluate_on_dataset(dataset['test_r2'])\n",
    "pred_test_r3 = evaluate_on_dataset(dataset['test_r3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0aa777f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'premise': 'Ernest Jones is a British jeweller and watchmaker. Established in 1949, its first store was opened in Oxford Street, London. Ernest Jones specialises in diamonds and watches, stocking brands such as Gucci and Emporio Armani. Ernest Jones is part of the Signet Jewelers group.',\n",
       "  'hypothesis': 'The first Ernest Jones store was opened on the continent of Europe.',\n",
       "  'prediction': {'entailment': 99.5, 'neutral': 0.1, 'contradiction': 0.3},\n",
       "  'pred_label': 'entailment',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': \"The first store was opened in London, which is in Europe. It may have been difficult for the system because continents weren't mentioned.\"},\n",
       " {'premise': 'Old Trafford is a football stadium in Old Trafford, Greater Manchester, England, and the home of Manchester United. With a capacity of 75,643, it is the largest club football stadium in the United Kingdom, the second-largest football stadium, and the eleventh-largest in Europe. It is about 0.5 mi from Old Trafford Cricket Ground and the adjacent tram stop.',\n",
       "  'hypothesis': 'There are only 10 larger football stadiums in Europe.',\n",
       "  'prediction': {'entailment': 58.6, 'neutral': 15.4, 'contradiction': 26.0},\n",
       "  'pred_label': 'entailment',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The text says that it is the 11th largest football stadium in Europe.'},\n",
       " {'premise': 'Magnus is a Belgian joint dance project of Tom Barman (from the rock band dEUS) and CJ Bolland. Magnus\\' debut album, \"The Body Gave You Everything\", was released on March 29, 2004. Two of its tracks, \"Summer\\'s Here\" and \"Jumpneedle\", were released as singles.',\n",
       "  'hypothesis': '\"The body gave you everything\" album was not released on March 28, 2003 but on March 29, 2004.',\n",
       "  'prediction': {'entailment': 95.7, 'neutral': 3.5, 'contradiction': 0.8},\n",
       "  'pred_label': 'entailment',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'it was released on March 29, 2004. \"not this but that\" type of sentences always confuse the system'},\n",
       " {'premise': \"Shadowboxer is a 2005 crime thriller film directed by Lee Daniels and starring Academy Award winners Cuba Gooding Jr., Helen Mirren, and Mo'Nique. It opened in limited release in six cities: New York, Los Angeles, Washington, D.C., Baltimore, Philadelphia, and Richmond, Virginia.\",\n",
       "  'hypothesis': \"Shadowboxer was written and directed by Lee Daniels and was starring Academy Award winners Cuba Gooding Jr., Helen Mirren, and Mo'Nique.\",\n",
       "  'prediction': {'entailment': 53.2, 'neutral': 46.1, 'contradiction': 0.8},\n",
       "  'pred_label': 'entailment',\n",
       "  'gold_label': 'neutral',\n",
       "  'reason': 'It is not know who wrote the Shadowboxer. The system can get confused if a small detail is added for a person while many correct details are written.'},\n",
       " {'premise': 'Takaaki Kajita (梶田 隆章 , Kajita Takaaki ) is a Japanese physicist, known for neutrino experiments at the Kamiokande and its successor, Super-Kamiokande. In 2015, he was awarded the Nobel Prize in Physics jointly with Canadian physicist Arthur B. McDonald.',\n",
       "  'hypothesis': 'Arthur B. McDonald is a Japanese physicist, known for neutrino experiments at the Kamiokande and its successor, Super-Kamiokande.',\n",
       "  'prediction': {'entailment': 0.2, 'neutral': 1.4, 'contradiction': 98.5},\n",
       "  'pred_label': 'contradiction',\n",
       "  'gold_label': 'contradiction',\n",
       "  'reason': 'Arthur B. McDonald is Canadian in the context.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_r1[:5] # Display the first 5 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcdf4616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'premise': 'There is a little Shia community in El Salvador. There is an Islamic Library operated by the Shia community, named \"Fatimah Az-Zahra\". They published the first Islamic magazine in Central America: \"Revista Biblioteca Islámica\". Additionally, they are credited with providing the first and only Islamic library dedicated to spreading Islamic culture in the country.',\n",
       "  'hypothesis': 'The community is south of the United States.',\n",
       "  'prediction': {'entailment': 94.5, 'neutral': 1.7, 'contradiction': 3.8},\n",
       "  'pred_label': 'entailment',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The community is in El Salvador which is south of the US.'},\n",
       " {'premise': '\"Look at Me (When I Rock Wichoo)\" is a song by American indie rock band Black Kids, taken from their debut album \"Partie Traumatic\". It was released in the UK by Almost Gold Recordings on September 8, 2008 and debuted on the Top 200 UK Singles Chart at number 175.',\n",
       "  'hypothesis': 'The song was released in America in September 2008',\n",
       "  'prediction': {'entailment': 3.1, 'neutral': 7.5, 'contradiction': 89.5},\n",
       "  'pred_label': 'contradiction',\n",
       "  'gold_label': 'neutral',\n",
       "  'reason': \"It doesn't state if it was released anywhere outside of the UK\"},\n",
       " {'premise': '\"Eternally\" is a song with music by Charles Chaplin, and words by the English lyricists Geoff Parsons and John Turner. The music was initially composed for Charles Chaplin\\'s film \"Limelight\" (1952) titled \"Terry\\'s Theme\"; the film won an Oscar for \"Best Original Dramatic Score\" at the',\n",
       "  'hypothesis': 'The words to Eternally were written partially by Geoff Parsons',\n",
       "  'prediction': {'entailment': 99.7, 'neutral': 0.2, 'contradiction': 0.1},\n",
       "  'pred_label': 'entailment',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'This is correct because it says him and another wrote the words. I think the system misunderstood the word partially'},\n",
       " {'premise': 'Louis S. Peterson (June 17, 1922 – April 27, 1998) was a playwright, actor, screenwriter, and professor. He was an American playwright and the first African-American playwright to have a dramatic play produced on Broadway. He was also one of the first African-American writers to be nominated for an Emmy Award.',\n",
       "  'hypothesis': 'Louis S. Peterson was an adult when he wrote his first play.',\n",
       "  'prediction': {'entailment': 0.9, 'neutral': 98.5, 'contradiction': 0.6},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'neutral',\n",
       "  'reason': 'The article does not state his age. Unsure why the system doesnt know this , maybe the word adult.'},\n",
       " {'premise': 'Things Happen at Night is a 1947 British supernatural ghost comedy film directed by Francis Searle and starring Gordon Harker, Alfred Drayton, Robertson Hare and Gwynneth Vaughan. The film is based upon a stage play, \"The Poltergeist\", by Frank Harvey Jnr.',\n",
       "  'hypothesis': 'Frank Harvey Jnr. wrote Things Happen at Night .',\n",
       "  'prediction': {'entailment': 12.5, 'neutral': 15.4, 'contradiction': 72.1},\n",
       "  'pred_label': 'contradiction',\n",
       "  'gold_label': 'contradiction',\n",
       "  'reason': 'It is based off of the play, but he did not actually write it. The system is unable to figure out one way or another.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_r2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8efb717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'premise': \"It is Sunday today, let's take a look at the most popular posts of the last couple of days. Most of the articles this week deal with the iPhone, its future version called the iPhone 8 or iPhone Edition, and new builds of iOS and macOS. There are also some posts that deal with the iPhone rival called the Galaxy S8 and some other interesting stories. The list of the most interesting articles is available below. Stay tuned for more rumors and don't forget to follow us on Twitter.\",\n",
       "  'hypothesis': 'The day of the passage is usually when Christians praise the lord together',\n",
       "  'prediction': {'entailment': 2.4, 'neutral': 97.4, 'contradiction': 0.2},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': \"Sunday is considered Lord's Day\"},\n",
       " {'premise': 'By The Associated Press WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. Copyright © 2018 The Associated Press. All rights reserved. This material may not be published, broadcast, written or redistributed.',\n",
       "  'hypothesis': 'No children were killed in the accident.',\n",
       "  'prediction': {'entailment': 0.1, 'neutral': 99.9, 'contradiction': 0.0},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The context confirms that everybody survived the accident, so there is no way that a child was killed.'},\n",
       " {'premise': 'Tokyo - Food group Nestle is seeking to lure Japanese holiday shoppers with a taste for fine snacking with a gold-wrapped Kit Kat chocolate bar. The single finger Kit Kat is wrapped in a thin layer of gold leaf. Only 500 of the bars go on sale from Dec. 29 with a price tag of around 2,016 yen ($16). The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced.',\n",
       "  'hypothesis': 'Japanese like kit kat. ',\n",
       "  'prediction': {'entailment': 84.0, 'neutral': 15.9, 'contradiction': 0.1},\n",
       "  'pred_label': 'entailment',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'according to the text, The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced, which means if  they have been so many produced it is because they like it. '},\n",
       " {'premise': 'Governor Greg Abbott has called for a statewide show of support for law enforcement Friday, July 7. Locally, a 15-minute program is planned at 9 a.m. at Memorial Lane Park, 550 N. Travis St. The governor is asking law enforcement officers to turn on red and blue flashing lights for one-minute at 10 a.m. Multiple law enforcement officers were shot and killed in Dallas one year ago.',\n",
       "  'hypothesis': 'Law enforcement officers and the people at the Travis St. memorial do not show their support at the same time.',\n",
       "  'prediction': {'entailment': 11.9, 'neutral': 75.8, 'contradiction': 12.3},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The Travis St.memorial program begins at 9 a.m. Law enforcement officers were asked to turn on red and blue flashing lights for one-minute at 10 a.m.'},\n",
       " {'premise': 'Sept 4 (Reuters) - J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France. Based in Paris, Grassano started in his new role on Sept. 1, J.P. Morgan Asset Management said in a statement. Grassano, who has been with the company since 2002, was previously the head of sales for Italy, covering wholesale and retail distribution. He has earlier worked at BNP Paribas Asset Management.',\n",
       "  'hypothesis': 'Pietro Grassano was once the country head for France.',\n",
       "  'prediction': {'entailment': 2.9, 'neutral': 55.1, 'contradiction': 42.0},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': '\"J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France.\" I think it was difficult because I worded it past tense, \"He was ONCE the country head\", but I believe that statement is true because it is past Sept 1 when he was appointed.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_r3[:5]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed48e1",
   "metadata": {},
   "source": [
    "### Task 1.2 - Investigate Errors of the NLI Model\n",
    "\n",
    "Sample 20 errors from the baseline model, and investigate the reasons the model made a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47f144df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 3200\n",
      "Total errors: 1347\n",
      "Error rate: 0.421\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "all_predictions = pred_test_r1 + pred_test_r2 + pred_test_r3\n",
    "\n",
    "errors = [pred for pred in all_predictions if pred['pred_label'] != pred['gold_label']]\n",
    "\n",
    "print(f\"Total predictions: {len(all_predictions)}\")\n",
    "print(f\"Total errors: {len(errors)}\")\n",
    "print(f\"Error rate: {len(errors)/len(all_predictions):.3f}\")\n",
    "\n",
    "# Sample 20 random errors\n",
    "random.seed(42)\n",
    "sampled_errors = random.sample(errors, min(20, len(errors)))\n",
    "\n",
    "error_analysis = []\n",
    "for i, error in enumerate(sampled_errors):\n",
    "    pred_label = error['pred_label']\n",
    "    gold_label = error['gold_label']\n",
    "    premise = error['premise']\n",
    "    hypothesis = error['hypothesis']\n",
    "    reason = error['reason']\n",
    "    \n",
    "    error_type = f\"{gold_label} → {pred_label}\"\n",
    "    \n",
    "    # Length analysis\n",
    "    premise_len = len(premise.split())\n",
    "    hypothesis_len = len(hypothesis.split())\n",
    "    \n",
    "    # Check for negation\n",
    "    has_negation = any(neg in premise.lower() or neg in hypothesis.lower() \n",
    "                      for neg in ['not', 'no', 'never', 'none', 'nobody', 'nothing', 'neither'])\n",
    "    \n",
    "    # Check for complex reasoning\n",
    "    has_complex_reasoning = any(word in premise.lower() or word in hypothesis.lower() \n",
    "                               for word in ['because', 'since', 'therefore', 'however', 'although', 'unless'])\n",
    "    \n",
    "    error_analysis.append({\n",
    "        'Error_ID': i+1,\n",
    "        'Error_Type': error_type,\n",
    "        'Premise': premise[:100] + \"...\" if len(premise) > 100 else premise,\n",
    "        'Hypothesis': hypothesis[:100] + \"...\" if len(hypothesis) > 100 else hypothesis,\n",
    "        'Gold_Label': gold_label,\n",
    "        'Pred_Label': pred_label,\n",
    "        'Premise_Length': premise_len,\n",
    "        'Hypothesis_Length': hypothesis_len,\n",
    "        'Has_Negation': has_negation,\n",
    "        'Complex_Reasoning': has_complex_reasoning,\n",
    "        'Human_Reason': reason[:150] + \"...\" if len(reason) > 150 else reason\n",
    "    })\n",
    "\n",
    "df_errors = pd.DataFrame(error_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64a70771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error_ID</th>\n",
       "      <th>Error_Type</th>\n",
       "      <th>Premise</th>\n",
       "      <th>Hypothesis</th>\n",
       "      <th>Gold_Label</th>\n",
       "      <th>Pred_Label</th>\n",
       "      <th>Premise_Length</th>\n",
       "      <th>Hypothesis_Length</th>\n",
       "      <th>Has_Negation</th>\n",
       "      <th>Complex_Reasoning</th>\n",
       "      <th>Human_Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>entailment → neutral</td>\n",
       "      <td>The National Park Trust identified 20 high-pri...</td>\n",
       "      <td>There are areas with property that could be so...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>neutral</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>There were at least 20 states, because the 20 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>contradiction → neutral</td>\n",
       "      <td>Gustave Marie Maurice Mesny (28 March 1886 – 1...</td>\n",
       "      <td>Gustave was a Brigadier General.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>neutral</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Gustave Mensy was a General. Completely separa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>entailment → neutral</td>\n",
       "      <td>The Face of Medusa (Greek: Το πρόσωπο της Μέδο...</td>\n",
       "      <td>The Face of Medusa was filmed in Europe</td>\n",
       "      <td>entailment</td>\n",
       "      <td>neutral</td>\n",
       "      <td>54</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>It is a Greek film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>neutral → contradiction</td>\n",
       "      <td>Johns Creek is a city located in Fulton County...</td>\n",
       "      <td>Johns Creek has a population of 92,000.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>52</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>The information from the census said here is 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>neutral → contradiction</td>\n",
       "      <td>Patricia Donoho Hughes (August 18, 1930 – Janu...</td>\n",
       "      <td>Particia Donoho Hughes was the First Lady of M...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Patricia Donoho Hughes died in 2010 but it's u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>entailment → neutral</td>\n",
       "      <td>The Local Government (Northern Ireland) Act 19...</td>\n",
       "      <td>Local authority was removed in favor of locali...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>neutral</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>The LGA Act abolished local authority and repl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>entailment → neutral</td>\n",
       "      <td>Choi Min-sik ( ] ; born January 22, 1962) is a...</td>\n",
       "      <td>Choi Min-sik has been an actor for at least 10...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>neutral</td>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>My statement is correct because Choi Min-sik h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>neutral → contradiction</td>\n",
       "      <td>The Face of Medusa (Greek: Το πρόσωπο της Μέδο...</td>\n",
       "      <td>The Face of Medusa is a 1967 Greek drama film ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>54</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Its not known who starred in the film. The sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>contradiction → neutral</td>\n",
       "      <td>For those who have just now picked up the deba...</td>\n",
       "      <td>the intended audience is long term followers t...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>neutral</td>\n",
       "      <td>57</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>The speaker is addressing new followers as the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>entailment → neutral</td>\n",
       "      <td>Sir is an honorific address used in a number o...</td>\n",
       "      <td>Baronets were called sir in their day.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>neutral</td>\n",
       "      <td>52</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>The prompt shows that people who were knights ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>entailment → neutral</td>\n",
       "      <td>The 2017 EFL League Two play-off Final was a f...</td>\n",
       "      <td>The promotion lasted a year</td>\n",
       "      <td>entailment</td>\n",
       "      <td>neutral</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>The promotion lasted for the 2017-18 year whic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>entailment → neutral</td>\n",
       "      <td>The Walkie Talkie&lt;br&gt;The boys loved playing ou...</td>\n",
       "      <td>The walkie talkies made it easy for the boys t...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>neutral</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>The whole context is about how the boys used t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>entailment → neutral</td>\n",
       "      <td>A scrum machine, or scrummaging machine, is a ...</td>\n",
       "      <td>At least one specific type of machine was succ...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>neutral</td>\n",
       "      <td>49</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>The scrum machine is a safe tool developed to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>neutral → contradiction</td>\n",
       "      <td>Melissa Duck is an animated cartoon character ...</td>\n",
       "      <td>Richman voiced Daffy Duck in several cartoons</td>\n",
       "      <td>neutral</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>We don't know who voiced Daffy Duck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>neutral → entailment</td>\n",
       "      <td>Emma Catherine Rigby (born 26 September 1989) ...</td>\n",
       "      <td>Emma Catherine Rigby was born in England in 1989.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>entailment</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>This statement is neither definitely correct n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>contradiction → entailment</td>\n",
       "      <td>The Wolfsonian–Florida International Universit...</td>\n",
       "      <td>For the total amount of years that is equivale...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>53</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>five times five equals 25 but the correct answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>neutral → entailment</td>\n",
       "      <td>O lieb, so lang du lieben kannst is a poem wri...</td>\n",
       "      <td>Dreams of Love showcased more than one musical...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>entailment</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>While it states that Dreams of Love had Liebes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>entailment → neutral</td>\n",
       "      <td>I have no problem with the people on welfare h...</td>\n",
       "      <td>Changes to employment insurance by the governm...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>neutral</td>\n",
       "      <td>78</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>The text states the government made changes to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>contradiction → entailment</td>\n",
       "      <td>The Anubis Shrine was part of the grave gods o...</td>\n",
       "      <td>The shrine was part of the gods of hamun</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>The gods of hamun do not exist making the stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>contradiction → entailment</td>\n",
       "      <td>Askold Anatolievich Makarov (Russian: Аско́льд...</td>\n",
       "      <td>Askod Makarov was a Russian ball dancer and te...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>61</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Askod Makarov was a ballet dancer not a ball d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Error_ID  ...                                       Human_Reason\n",
       "0          1  ...  There were at least 20 states, because the 20 ...\n",
       "1          2  ...  Gustave Mensy was a General. Completely separa...\n",
       "2          3  ...                                 It is a Greek film\n",
       "3          4  ...  The information from the census said here is 9...\n",
       "4          5  ...  Patricia Donoho Hughes died in 2010 but it's u...\n",
       "5          6  ...  The LGA Act abolished local authority and repl...\n",
       "6          7  ...  My statement is correct because Choi Min-sik h...\n",
       "7          8  ...  Its not known who starred in the film. The sys...\n",
       "8          9  ...  The speaker is addressing new followers as the...\n",
       "9         10  ...  The prompt shows that people who were knights ...\n",
       "10        11  ...  The promotion lasted for the 2017-18 year whic...\n",
       "11        12  ...  The whole context is about how the boys used t...\n",
       "12        13  ...  The scrum machine is a safe tool developed to ...\n",
       "13        14  ...                We don't know who voiced Daffy Duck\n",
       "14        15  ...  This statement is neither definitely correct n...\n",
       "15        16  ...  five times five equals 25 but the correct answ...\n",
       "16        17  ...  While it states that Dreams of Love had Liebes...\n",
       "17        18  ...  The text states the government made changes to...\n",
       "18        19  ...  The gods of hamun do not exist making the stat...\n",
       "19        20  ...  Askod Makarov was a ballet dancer not a ball d...\n",
       "\n",
       "[20 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_errors.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1727c80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ERROR ANALYSIS TABLE ===\n",
      " Error_ID                 Error_Type                                                                                                 Premise                                                                                              Hypothesis    Gold_Label    Pred_Label  Premise_Length  Hypothesis_Length  Has_Negation  Complex_Reasoning                                                                                                                                              Human_Reason\n",
      "        1       entailment → neutral The National Park Trust identified 20 high-priority sites - including the Blue Ridge Parkway in Nort...                                         There are areas with property that could be sold in many states    entailment       neutral              35                 12          True              False There were at least 20 states, because the 20 is only considered high priority, there are probably more, which means the word \"many\" is probably appli...\n",
      "        2    contradiction → neutral Gustave Marie Maurice Mesny (28 March 1886 – 19 January 1945) was a French Army general in command o...                                                                       Gustave was a Brigadier General.  contradiction       neutral              48                  5          True              False                                                                                Gustave Mensy was a General. Completely separate from a Brigadier General.\n",
      "        3       entailment → neutral The Face of Medusa (Greek: Το πρόσωπο της Μέδουσας , translit. To prosopo tis Medousas and also know...                                                                 The Face of Medusa was filmed in Europe    entailment       neutral              54                  8          True              False                                                                                                                                        It is a Greek film\n",
      "        4    neutral → contradiction Johns Creek is a city located in Fulton County in the U.S. state of Georgia. According to the 2010 U...                                                                 Johns Creek has a population of 92,000.       neutral contradiction              52                  7          True              False            The information from the census said here is 9 years old, so we do not know the population now. The system is taking the 2010 data as current.\n",
      "        5    neutral → contradiction Patricia Donoho Hughes (August 18, 1930 – January 20, 2010) was a First Lady of Maryland, married to...                                          Particia Donoho Hughes was the First Lady of Maryland in 2010.       neutral contradiction              61                 11          True              False Patricia Donoho Hughes died in 2010 but it's unclear whether she was still the First Lady upon her death. It's possible she still was but it's not cer...\n",
      "        6       entailment → neutral The Local Government (Northern Ireland) Act 1972 (1972 c. 9) was an Act of the Parliament of Norther...                                           Local authority was removed in favor of localized governance     entailment       neutral              49                  9          True              False     The LGA Act abolished local authority and replaced with 26 local government districts. The model probably couldn't tell what 'governance' referred to\n",
      "        7       entailment → neutral Choi Min-sik ( ] ; born January 22, 1962) is a South Korean actor. He is best known for his critical...                                                   Choi Min-sik has been an actor for at least 10 years.    entailment       neutral              46                 11          True              False My statement is correct because Choi Min-sik had movies in 2003 and 2014, meaning his acting career was definitely at least 10 years. The system was p...\n",
      "        8    neutral → contradiction The Face of Medusa (Greek: Το πρόσωπο της Μέδουσας , translit. To prosopo tis Medousas and also know...                         The Face of Medusa is a 1967 Greek drama film that starred by Nikos Koundouros.       neutral contradiction              54                 15          True              False Its not known who starred in the film. The system is confused when a person that is in the text is given to do something different than what is descri...\n",
      "        9    contradiction → neutral For those who have just now picked up the debate, we are dealing with the fact the Reform Party thin...                             the intended audience is long term followers that the speaker is addressing contradiction       neutral              57                 12          True              False The speaker is addressing new followers as the phrase \"for those who have just now picked up the debate ' implies. This makes my statement incorrect. ...\n",
      "       10       entailment → neutral Sir is an honorific address used in a number of situations in many anglophone cultures. The term can...                                                                  Baronets were called sir in their day.    entailment       neutral              52                  7          True              False                                                                                The prompt shows that people who were knights or baronets were called sir.\n",
      "       11       entailment → neutral The 2017 EFL League Two play-off Final was a football match that was contested between Blackpool and...                                                                             The promotion lasted a year    entailment       neutral              45                  5         False              False The promotion lasted for the 2017-18 year which is one year making my statement correct. I am guessing the model got confused by the way I phrased the...\n",
      "       12       entailment → neutral The Walkie Talkie<br>The boys loved playing outside. They had walkie talkies that the used to talk t...                        The walkie talkies made it easy for the boys to keep in contact with each other     entailment       neutral              47                 16         False              False The whole context is about how the boys used the walkie talkies to talk to each other the whole summer. I think the system didn't guess right because ...\n",
      "       13       entailment → neutral A scrum machine, or scrummaging machine, is a padded, weighty device against which a pack of rugby f...       At least one specific type of machine was successfully developed to make practicing rugby safer.     entailment       neutral              49                 15         False              False                                                                                  The scrum machine is a safe tool developed to make rugby practice safer.\n",
      "       14    neutral → contradiction Melissa Duck is an animated cartoon character in the Warner Brothers \"Looney Tunes\" and \"Merrie Melo...                                                           Richman voiced Daffy Duck in several cartoons       neutral contradiction              60                  7         False              False                                                                                                                       We don't know who voiced Daffy Duck\n",
      "       15       neutral → entailment Emma Catherine Rigby (born 26 September 1989) is an English actress. She is best known for playing t...                                                       Emma Catherine Rigby was born in England in 1989.       neutral    entailment              50                  9          True              False This statement is neither definitely correct nor definitely incorrect because although the information provided indicates Emma Catherine Rigby is Engl...\n",
      "       16 contradiction → entailment The Wolfsonian–Florida International University or The Wolfsonian-FIU, located in the heart of the A... For the total amount of years that is equivalent to five multiplied by five, The Wolfsonian has been... contradiction    entailment              53                 24         False              False five times five equals 25 but the correct answer is fifteen so it was incorrect but the AI recognized the wording but not able to calculate to see if ...\n",
      "       17       neutral → entailment O lieb, so lang du lieben kannst is a poem written by Ferdinand Freiligrath, a 19th-century German w...                                                   Dreams of Love showcased more than one musical number       neutral    entailment              60                  9          True              False While it states that Dreams of Love had Liebestraume No 3, it did not state it had other numbers in it. Composers often stick a piece labeled 'number ...\n",
      "       18       entailment → neutral I have no problem with the people on welfare having an opportunity for a job, but I do not agree wit...                          Changes to employment insurance by the government had unforeseen consequences.    entailment       neutral              78                 10          True               True                                       The text states the government made changes to employment insurance that increased the number of people on welfare.\n",
      "       19 contradiction → entailment The Anubis Shrine was part of the grave gods of Tutankhamun (18th Dynasty, New Kingdom). The tomb (K...                                                                The shrine was part of the gods of hamun contradiction    entailment              61                  9          True              False                                         The gods of hamun do not exist making the statement incorrect. The model has problems with the spellings of words\n",
      "       20 contradiction → entailment Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000...                             Askod Makarov was a Russian ball dancer and teacher in the 1960's and 70's. contradiction    entailment              61                 14         False              False                                                                                                      Askod Makarov was a ballet dancer not a ball dancer.\n",
      "\n",
      "=== ERROR SUMMARY ===\n",
      "Error type distribution:\n",
      "Error_Type\n",
      "entailment → neutral          9\n",
      "neutral → contradiction       4\n",
      "contradiction → entailment    3\n",
      "contradiction → neutral       2\n",
      "neutral → entailment          2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average premise length: 53.6 words\n",
      "Average hypothesis length: 10.8 words\n",
      "Errors with negation: 14/20 (70.0%)\n",
      "Errors with complex reasoning: 1/20 (5.0%)\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame for better display\n",
    "df_errors = pd.DataFrame(error_analysis)\n",
    "print(\"\\n=== ERROR ANALYSIS TABLE ===\")\n",
    "print(df_errors.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n=== ERROR SUMMARY ===\")\n",
    "print(f\"Error type distribution:\")\n",
    "error_type_counts = df_errors['Error_Type'].value_counts()\n",
    "print(error_type_counts)\n",
    "\n",
    "print(f\"\\nAverage premise length: {df_errors['Premise_Length'].mean():.1f} words\")\n",
    "print(f\"Average hypothesis length: {df_errors['Hypothesis_Length'].mean():.1f} words\")\n",
    "print(f\"Errors with negation: {df_errors['Has_Negation'].sum()}/20 ({df_errors['Has_Negation'].sum()/20*100:.1f}%)\")\n",
    "print(f\"Errors with complex reasoning: {df_errors['Complex_Reasoning'].sum()}/20 ({df_errors['Complex_Reasoning'].sum()/20*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline model on each section of the ANLI dataset.\n",
    "\n",
    "https://www.kaggle.com/code/faijanahamadkhan/llm-evaluation-framework-hugging-face provides good documentation on how to use the Huggingface evaluate library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fbc81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19904d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST_R1 METRICS ===\n",
      "Accuracy: 0.7120\n",
      "F1 Score: 0.7119\n",
      "Precision: 0.7135\n",
      "Recall: 0.7120\n",
      "Total samples: 1000\n",
      "\n",
      "=== TEST_R2 METRICS ===\n",
      "Accuracy: 0.5470\n",
      "F1 Score: 0.5465\n",
      "Precision: 0.5472\n",
      "Recall: 0.5470\n",
      "Total samples: 1000\n",
      "\n",
      "=== TEST_R3 METRICS ===\n",
      "Accuracy: 0.4950\n",
      "F1 Score: 0.4945\n",
      "Precision: 0.4984\n",
      "Recall: 0.4950\n",
      "Total samples: 1200\n",
      "\n",
      "=== OVERALL METRICS ===\n",
      "Accuracy: 0.5791\n",
      "F1 Score: 0.5786\n",
      "Precision: 0.5805\n",
      "Recall: 0.5791\n",
      "Total samples: 3200\n",
      "\n",
      "=== SUMMARY TABLE ===\n",
      "Section  Accuracy     F1  Precision  Recall  Samples\n",
      "Test_R1    0.7120 0.7119     0.7135  0.7120     1000\n",
      "Test_R2    0.5470 0.5465     0.5472  0.5470     1000\n",
      "Test_R3    0.4950 0.4945     0.4984  0.4950     1200\n",
      "Overall    0.5791 0.5786     0.5805  0.5791     3200\n",
      "\n",
      "=== PER-CLASS METRICS (Overall) ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment     0.5983    0.5245    0.5590      1062\n",
      "      neutral     0.5950    0.6178    0.6061      1070\n",
      "contradiction     0.5484    0.5946    0.5705      1068\n",
      "\n",
      "     accuracy                         0.5791      3200\n",
      "    macro avg     0.5805    0.5789    0.5785      3200\n",
      " weighted avg     0.5805    0.5791    0.5786      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def labels_to_numeric(labels):\n",
    "    label_to_num = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "    return [label_to_num[label] for label in labels]\n",
    "\n",
    "def compute_metrics_for_section(predictions, section_name):\n",
    "    pred_labels = [pred['pred_label'] for pred in predictions]\n",
    "    gold_labels = [pred['gold_label'] for pred in predictions]\n",
    "    \n",
    "    pred_numeric = labels_to_numeric(pred_labels)\n",
    "    gold_numeric = labels_to_numeric(gold_labels)\n",
    "    \n",
    "    acc = accuracy.compute(predictions=pred_numeric, references=gold_numeric)\n",
    "    prec = precision.compute(predictions=pred_numeric, references=gold_numeric, average='weighted')\n",
    "    rec = recall.compute(predictions=pred_numeric, references=gold_numeric, average='weighted')\n",
    "    f1_score = f1.compute(predictions=pred_numeric, references=gold_numeric, average='weighted')\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': acc['accuracy'],\n",
    "        'precision': prec['precision'],\n",
    "        'recall': rec['recall'],\n",
    "        'f1': f1_score['f1']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== {section_name} METRICS ===\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"Total samples: {len(predictions)}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Compute metrics for each test section\n",
    "metrics_r1 = compute_metrics_for_section(pred_test_r1, \"TEST_R1\")\n",
    "metrics_r2 = compute_metrics_for_section(pred_test_r2, \"TEST_R2\") \n",
    "metrics_r3 = compute_metrics_for_section(pred_test_r3, \"TEST_R3\")\n",
    "\n",
    "all_test_predictions = pred_test_r1 + pred_test_r2 + pred_test_r3\n",
    "metrics_overall = compute_metrics_for_section(all_test_predictions, \"OVERALL\")\n",
    "\n",
    "# Create summary table\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = {\n",
    "    'Section': ['Test_R1', 'Test_R2', 'Test_R3', 'Overall'],\n",
    "    'Accuracy': [metrics_r1['accuracy'], metrics_r2['accuracy'], metrics_r3['accuracy'], metrics_overall['accuracy']],\n",
    "    'F1': [metrics_r1['f1'], metrics_r2['f1'], metrics_r3['f1'], metrics_overall['f1']],\n",
    "    'Precision': [metrics_r1['precision'], metrics_r2['precision'], metrics_r3['precision'], metrics_overall['precision']],\n",
    "    'Recall': [metrics_r1['recall'], metrics_r2['recall'], metrics_r3['recall'], metrics_overall['recall']],\n",
    "    'Samples': [len(pred_test_r1), len(pred_test_r2), len(pred_test_r3), len(all_test_predictions)]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\n=== SUMMARY TABLE ===\")\n",
    "print(df_summary.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Additional analysis: per-class metrics\n",
    "print(\"\\n=== PER-CLASS METRICS (Overall) ===\")\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "all_pred_labels = [pred['pred_label'] for pred in all_test_predictions]\n",
    "all_gold_labels = [pred['gold_label'] for pred in all_test_predictions]\n",
    "\n",
    "print(classification_report(all_gold_labels, all_pred_labels, \n",
    "                          target_names=['entailment', 'neutral', 'contradiction'],\n",
    "                          digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "136102c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_R1</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.711924</td>\n",
       "      <td>0.713468</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_R2</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0.546530</td>\n",
       "      <td>0.547205</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_R3</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.494467</td>\n",
       "      <td>0.498436</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Overall</td>\n",
       "      <td>0.579063</td>\n",
       "      <td>0.578597</td>\n",
       "      <td>0.580509</td>\n",
       "      <td>0.579063</td>\n",
       "      <td>3200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Section  Accuracy        F1  Precision    Recall  Samples\n",
       "0  Test_R1  0.712000  0.711924   0.713468  0.712000     1000\n",
       "1  Test_R2  0.547000  0.546530   0.547205  0.547000     1000\n",
       "2  Test_R3  0.495000  0.494467   0.498436  0.495000     1200\n",
       "3  Overall  0.579063  0.578597   0.580509  0.579063     3200"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
