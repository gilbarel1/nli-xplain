{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres Baseline\n",
    "\n",
    "This notebook illustrates how to use the DeBERTa-v3-base-mnli-fever-anli model to perform specialized inference on the ImpPres dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fdf48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfe31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "def evaluate(premise, hypothesis):\n",
    "    input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(input[\"input_ids\"].to(device))\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2954d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'entailment': 0.1, 'neutral': 99.8, 'contradiction': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"The weather is nice today.\", \"It is sunny outside.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923ea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(pred_dict):\n",
    "    return max(pred_dict, key=pred_dict.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ImpPres Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0438789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for section: presupposition_all_n_presupposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for section: presupposition_both_presupposition\n",
      "Loading dataset for section: presupposition_change_of_state\n",
      "Loading dataset for section: presupposition_cleft_existence\n",
      "Loading dataset for section: presupposition_cleft_uniqueness\n",
      "Loading dataset for section: presupposition_only_presupposition\n",
      "Loading dataset for section: presupposition_possessed_definites_existence\n",
      "Loading dataset for section: presupposition_possessed_definites_uniqueness\n",
      "Loading dataset for section: presupposition_question_presupposition\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sections = ['presupposition_all_n_presupposition', \n",
    "            'presupposition_both_presupposition', \n",
    "            'presupposition_change_of_state', \n",
    "            'presupposition_cleft_existence', \n",
    "            'presupposition_cleft_uniqueness', \n",
    "            'presupposition_only_presupposition', \n",
    "            'presupposition_possessed_definites_existence', \n",
    "            'presupposition_possessed_definites_uniqueness', \n",
    "            'presupposition_question_presupposition']\n",
    "\n",
    "dataset = {}\n",
    "for section in sections:\n",
    "    print(f\"Loading dataset for section: {section}\")\n",
    "    dataset[section] = load_dataset(\"facebook/imppres\", section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presupposition_all_n_presupposition': DatasetDict({\n",
       "     all_n_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_both_presupposition': DatasetDict({\n",
       "     both_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_change_of_state': DatasetDict({\n",
       "     change_of_state: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_cleft_existence': DatasetDict({\n",
       "     cleft_existence: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_cleft_uniqueness': DatasetDict({\n",
       "     cleft_uniqueness: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_only_presupposition': DatasetDict({\n",
       "     only_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_possessed_definites_existence': DatasetDict({\n",
       "     possessed_definites_existence: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_possessed_definites_uniqueness': DatasetDict({\n",
       "     possessed_definites_uniqueness: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " }),\n",
       " 'presupposition_question_presupposition': DatasetDict({\n",
       "     question_presupposition: Dataset({\n",
       "         features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID'],\n",
       "         num_rows: 1900\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8262068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the ImpPres dataset\n",
    "from tqdm import tqdm\n",
    "def evaluate_on_dataset(dataset):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    for example in tqdm(dataset):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = evaluate(premise, hypothesis)\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'prediction': prediction,\n",
    "            'pred_label': get_prediction(prediction),\n",
    "            'gold_label': label_names[example['gold_label']],\n",
    "            'section': example['section']\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import combine\n",
    "\n",
    "clf_metrics = combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline model on each section of the ImpPres dataset.\n",
    "\n",
    "https://www.kaggle.com/code/faijanahamadkhan/llm-evaluation-framework-hugging-face provides good documentation on how to use the Huggingface evaluate library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a51fbc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'trigger', 'trigger1', 'trigger2', 'presupposition', 'gold_label', 'UID', 'pairID', 'paradigmID', 'section'],\n",
       "    num_rows: 17100\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "unified_pres = load_from_disk(\"unified_presupposition.hf\")\n",
    "unified_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5405e9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17100/17100 [10:01<00:00, 28.41it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_on_dataset(unified_pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc04a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from collections import defaultdict\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "macro_f1 = load(\"f1\")\n",
    "macro_precision = load(\"precision\")\n",
    "macro_recall = load(\"recall\")\n",
    "\n",
    "preds = defaultdict(list)\n",
    "refs = defaultdict(list)\n",
    "for res in results:\n",
    "    preds[res['section']].append(label_names.index(res['pred_label']))\n",
    "    refs[res['section']].append(label_names.index(res['gold_label']))\n",
    "\n",
    "\n",
    "classification_results = {}\n",
    "for section in preds:\n",
    "    classification_results[section] = (\n",
    "        accuracy.compute(predictions=preds[section], references=refs[section]) |\n",
    "        macro_f1.compute(predictions=preds[section], references=refs[section], average='macro') |\n",
    "        macro_precision.compute(predictions=preds[section], references=refs[section], average='macro') |\n",
    "        macro_recall.compute(predictions=preds[section], references=refs[section], average='macro')\n",
    "    )\n",
    "\n",
    "classification_results['total'] = (\n",
    "        accuracy.compute(predictions=[p for section in preds.values() for p in section], \n",
    "                        references=[r for section in refs.values() for r in section]) |\n",
    "        macro_f1.compute(predictions=[p for section in preds.values() for p in section], \n",
    "                        references=[r for section in refs.values() for r in section], average='macro') |\n",
    "        macro_precision.compute(predictions=[p for section in preds.values() for p in section], \n",
    "                                references=[r for section in refs.values() for r in section], average='macro') |\n",
    "        macro_recall.compute(predictions=[p for section in preds.values() for p in section], \n",
    "                            references=[r for section in refs.values() for r in section], average='macro')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4687294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Section",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Macro F1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Macro Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Macro Recall",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "37743303-3192-4885-8d5a-0130a3dd5e67",
       "rows": [
        [
         "0",
         "all_n_presupposition",
         "0.5405263157894736",
         "0.5056411575987084",
         "0.5168807401178611",
         "0.5298611111111111"
        ],
        [
         "1",
         "both_presupposition",
         "0.3605263157894737",
         "0.3044645252894244",
         "0.29452724650052536",
         "0.32688888888888884"
        ],
        [
         "2",
         "change_of_state",
         "0.4131578947368421",
         "0.40784071369185476",
         "0.41716850841009157",
         "0.40477777777777774"
        ],
        [
         "3",
         "cleft_existence",
         "0.6868421052631579",
         "0.6836108031317667",
         "0.7163752708768697",
         "0.7298888888888889"
        ],
        [
         "4",
         "cleft_uniqueness",
         "0.2231578947368421",
         "0.2041716221866098",
         "0.21130687299310566",
         "0.20555555555555557"
        ],
        [
         "5",
         "only_presupposition",
         "0.6778947368421052",
         "0.6764071078077799",
         "0.7018982966907936",
         "0.7144444444444445"
        ],
        [
         "6",
         "possessed_definites_existence",
         "0.7689473684210526",
         "0.7761021907937299",
         "0.8318560827457625",
         "0.8146666666666667"
        ],
        [
         "7",
         "possessed_definites_uniqueness",
         "0.3994736842105263",
         "0.3158709916700784",
         "0.28136602588125914",
         "0.3627777777777778"
        ],
        [
         "8",
         "question_presupposition",
         "0.7152631578947368",
         "0.7112510864419069",
         "0.765572415857705",
         "0.7659166666666667"
        ],
        [
         "9",
         "total",
         "0.5317543859649123",
         "0.5341013163270939",
         "0.545634510608955",
         "0.5394197530864197"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>0.540526</td>\n",
       "      <td>0.505641</td>\n",
       "      <td>0.516881</td>\n",
       "      <td>0.529861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>both_presupposition</td>\n",
       "      <td>0.360526</td>\n",
       "      <td>0.304465</td>\n",
       "      <td>0.294527</td>\n",
       "      <td>0.326889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>change_of_state</td>\n",
       "      <td>0.413158</td>\n",
       "      <td>0.407841</td>\n",
       "      <td>0.417169</td>\n",
       "      <td>0.404778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cleft_existence</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>0.683611</td>\n",
       "      <td>0.716375</td>\n",
       "      <td>0.729889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cleft_uniqueness</td>\n",
       "      <td>0.223158</td>\n",
       "      <td>0.204172</td>\n",
       "      <td>0.211307</td>\n",
       "      <td>0.205556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>only_presupposition</td>\n",
       "      <td>0.677895</td>\n",
       "      <td>0.676407</td>\n",
       "      <td>0.701898</td>\n",
       "      <td>0.714444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>possessed_definites_existence</td>\n",
       "      <td>0.768947</td>\n",
       "      <td>0.776102</td>\n",
       "      <td>0.831856</td>\n",
       "      <td>0.814667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>possessed_definites_uniqueness</td>\n",
       "      <td>0.399474</td>\n",
       "      <td>0.315871</td>\n",
       "      <td>0.281366</td>\n",
       "      <td>0.362778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>0.715263</td>\n",
       "      <td>0.711251</td>\n",
       "      <td>0.765572</td>\n",
       "      <td>0.765917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>total</td>\n",
       "      <td>0.531754</td>\n",
       "      <td>0.534101</td>\n",
       "      <td>0.545635</td>\n",
       "      <td>0.539420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Section  Accuracy  Macro F1  Macro Precision  \\\n",
       "0            all_n_presupposition  0.540526  0.505641         0.516881   \n",
       "1             both_presupposition  0.360526  0.304465         0.294527   \n",
       "2                 change_of_state  0.413158  0.407841         0.417169   \n",
       "3                 cleft_existence  0.686842  0.683611         0.716375   \n",
       "4                cleft_uniqueness  0.223158  0.204172         0.211307   \n",
       "5             only_presupposition  0.677895  0.676407         0.701898   \n",
       "6   possessed_definites_existence  0.768947  0.776102         0.831856   \n",
       "7  possessed_definites_uniqueness  0.399474  0.315871         0.281366   \n",
       "8         question_presupposition  0.715263  0.711251         0.765572   \n",
       "9                           total  0.531754  0.534101         0.545635   \n",
       "\n",
       "   Macro Recall  \n",
       "0      0.529861  \n",
       "1      0.326889  \n",
       "2      0.404778  \n",
       "3      0.729889  \n",
       "4      0.205556  \n",
       "5      0.714444  \n",
       "6      0.814667  \n",
       "7      0.362778  \n",
       "8      0.765917  \n",
       "9      0.539420  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(classification_results).T\n",
    "df = df.rename(columns={\n",
    "    'accuracy': 'Accuracy',\n",
    "    'f1': 'Macro F1',\n",
    "    'precision': 'Macro Precision',\n",
    "    'recall': 'Macro Recall'\n",
    "})\n",
    "df.index.name = 'Section'\n",
    "df.reset_index(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62f46ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "mistakes = []\n",
    "for result in results:\n",
    "    if result['pred_label'] != result['gold_label']:\n",
    "        mistakes.append(result)\n",
    "\n",
    "with open('imppres_deberta_mistakes.json', 'w') as f:\n",
    "    json.dump(mistakes, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2 (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
