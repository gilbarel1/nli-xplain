{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ImpPres LLM Baseline\n",
    "\n",
    "You have to implement in this notebook a baseline for ImpPres classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305417ca",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e53272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "unified_pres = load_from_disk(\"unified_presupposition.hf\")\n",
    "\n",
    "train_dataset = []\n",
    "test_dataset = []\n",
    "for item in unified_pres:\n",
    "    if item['paradigmID'] == 0:\n",
    "        train_dataset.append(item)\n",
    "    else:\n",
    "        test_dataset.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732068f1",
   "metadata": {},
   "source": [
    "## Implement DsPy Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d566d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class Presupposition(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Identify whether the premise entails, contradicts, or is neutral with respect to the hypothesis.\n",
    "    \"\"\"\n",
    "    premise: str = dspy.InputField(desc=\"A statement that is assumed to be true.\")\n",
    "    hypothesis: str = dspy.InputField(desc=\"The statement that is being evaluated in relation to the premise.\")\n",
    "    presupposes: Literal['entailment', 'contradiction', 'neutral'] = dspy.OutputField(desc=\"The relationship between the premise and hypothesis, indicating whether the premise entails, contradicts, or is neutral with respect to the hypothesis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66564633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entailment\n"
     ]
    }
   ],
   "source": [
    "pres = dspy.Predict(Presupposition)\n",
    "ans = pres(premise=\"The guest had found John.\", hypothesis=\"John used to be in an unknown location.\")\n",
    "print(ans.presupposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9357a120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The premise states that \"The guest had found John,\" which implies that John was previously in a location or state that was not known to the guest, as the act of finding someone typically involves discovering them after they were unknown or inaccessible. This directly supports the hypothesis that \"John used to be in an unknown location,\" making the premise entail the hypothesis.\n",
      "entailment\n"
     ]
    }
   ],
   "source": [
    "pres_cot = dspy.ChainOfThought(Presupposition)\n",
    "ans = pres_cot(premise=\"The guest had found John.\", hypothesis=\"John used to be in an unknown location.\")\n",
    "print(ans.reasoning)\n",
    "print(ans.presupposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b04e8b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:00<00:00, 859.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 4 examples for up to 3 rounds, amounting to 4 attempts.\n",
      "entailment\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from collections import defaultdict\n",
    "\n",
    "label_names = ['entailment', 'contradiction', 'neutral']\n",
    "\n",
    "def validate_answer(example, pred, trace=None):\n",
    "    \"\"\"Validation function for DSPy optimization\"\"\"\n",
    "    return example.presupposes == pred.presupposes\n",
    "\n",
    "# Configure the teleprompter for optimization\n",
    "teleprompter = BootstrapFewShot(\n",
    "    metric=validate_answer,\n",
    "    max_bootstrapped_demos=4,  # Number of examples to bootstrap\n",
    "    max_labeled_demos=8,       # Maximum number of labeled demonstrations\n",
    "    max_rounds=3               # Number of optimization rounds\n",
    ")\n",
    "\n",
    "def cut_data_round_robin(dataset, max_examples, complete_round=False):\n",
    "    section_to_items = defaultdict(list)\n",
    "    for item in dataset:\n",
    "        section_to_items[item['section']].append(item)\n",
    "\n",
    "    subset = []\n",
    "    i = 0\n",
    "    added = True\n",
    "    while added and len(subset) < max_examples:\n",
    "        added = False\n",
    "        for items in section_to_items.values():\n",
    "            if not complete_round and len(subset) == max_examples:\n",
    "                break\n",
    "            added = True\n",
    "            subset.append(items[i])\n",
    "        i += 1\n",
    "    return subset\n",
    "\n",
    "def prepare_dspy_examples(max_examples: int = 20):\n",
    "    \"\"\"Round robin sampling of examples from diffrent sections of the training dataset.\"\"\"\n",
    "    subset = cut_data_round_robin(train_dataset, max_examples)\n",
    "    return [dspy.Example(\n",
    "        premise=item['premise'],\n",
    "        hypothesis=item['hypothesis'],\n",
    "        presupposes=label_names[item['gold_label']]\n",
    "    ).with_inputs('premise', 'hypothesis') for item in subset]\n",
    "\n",
    "pres_fewshot = teleprompter.compile(pres, trainset=prepare_dspy_examples())\n",
    "ans = pres_fewshot(premise=\"The guest had found John.\", hypothesis=\"John used to be in an unknown location.\")\n",
    "print(ans.presupposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0a36d",
   "metadata": {},
   "source": [
    "### Standalone Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def evaluate_on_dataset(dataset, program):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    for example in tqdm(dataset):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = program(premise=premise, hypothesis=hypothesis)\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'reasoning': prediction.reasoning if hasattr(prediction, 'reasoning') else None,\n",
    "            'pred_label': prediction.presupposes,\n",
    "            'gold_label': label_names[example['gold_label']],\n",
    "            'section': example['section']\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10c7513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 513/513 [00:00<00:00, 3862.54it/s]\n",
      "100%|██████████| 513/513 [00:00<00:00, 3807.90it/s]\n",
      "100%|██████████| 513/513 [00:00<00:00, 1518.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on a subset of the test dataset (3 paradigms and all triggers for each section) to save time and money\n",
    "narrowed_test = cut_data_round_robin(test_dataset, 513, complete_round=True)\n",
    "results_vanilla = evaluate_on_dataset(narrowed_test, pres)\n",
    "results_cot = evaluate_on_dataset(narrowed_test, pres_cot)\n",
    "results_fewshot = evaluate_on_dataset(narrowed_test, pres_fewshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c485f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from collections import defaultdict\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "macro_f1 = load(\"f1\")\n",
    "macro_precision = load(\"precision\")\n",
    "macro_recall = load(\"recall\")\n",
    "\n",
    "preds = defaultdict(list)\n",
    "refs = defaultdict(list)\n",
    "\n",
    "model_classifications = []\n",
    "for results in [results_vanilla, results_cot, results_fewshot]:\n",
    "    for res in results:\n",
    "        preds[res['section']].append(label_names.index(res['pred_label']))\n",
    "        refs[res['section']].append(label_names.index(res['gold_label']))\n",
    "\n",
    "\n",
    "    classification_results = {}\n",
    "    for section in preds:\n",
    "        classification_results[section] = (\n",
    "            accuracy.compute(predictions=preds[section], references=refs[section]) |\n",
    "            macro_f1.compute(predictions=preds[section], references=refs[section], average='macro') |\n",
    "            macro_precision.compute(predictions=preds[section], references=refs[section], average='macro') |\n",
    "            macro_recall.compute(predictions=preds[section], references=refs[section], average='macro')\n",
    "        )\n",
    "\n",
    "    classification_results['total'] = (\n",
    "            accuracy.compute(predictions=[p for section in preds.values() for p in section], \n",
    "                            references=[r for section in refs.values() for r in section]) |\n",
    "            macro_f1.compute(predictions=[p for section in preds.values() for p in section], \n",
    "                            references=[r for section in refs.values() for r in section], average='macro') |\n",
    "            macro_precision.compute(predictions=[p for section in preds.values() for p in section], \n",
    "                                    references=[r for section in refs.values() for r in section], average='macro') |\n",
    "            macro_recall.compute(predictions=[p for section in preds.values() for p in section], \n",
    "                                references=[r for section in refs.values() for r in section], average='macro')\n",
    "        )\n",
    "\n",
    "    model_classifications.append(classification_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdb37505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "for classification_results in model_classifications:\n",
    "    df = pd.DataFrame(classification_results).T\n",
    "    df = df.rename(columns={\n",
    "        'accuracy': 'Accuracy',\n",
    "        'f1': 'Macro F1',\n",
    "        'precision': 'Macro Precision',\n",
    "        'recall': 'Macro Recall'\n",
    "    })\n",
    "    df.index.name = 'Section'\n",
    "    df.reset_index(inplace=True)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c741647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Section",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Macro F1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Macro Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Macro Recall",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "53daf121-7e0d-43ab-a033-b3801750a3dd",
       "rows": [
        [
         "0",
         "all_n_presupposition",
         "0.9473684210526315",
         "0.9492900608519269",
         "0.9629629629629629",
         "0.9407407407407407"
        ],
        [
         "1",
         "both_presupposition",
         "0.9824561403508771",
         "0.9836734693877552",
         "0.9866666666666667",
         "0.9814814814814815"
        ],
        [
         "2",
         "change_of_state",
         "0.5614035087719298",
         "0.46950415945968205",
         "0.6979905437352246",
         "0.49259259259259264"
        ],
        [
         "3",
         "cleft_existence",
         "0.7192982456140351",
         "0.6833333333333332",
         "0.8666666666666667",
         "0.6666666666666666"
        ],
        [
         "4",
         "cleft_uniqueness",
         "0.49122807017543857",
         "0.3446969696969697",
         "0.8176100628930817",
         "0.41111111111111115"
        ],
        [
         "5",
         "only_presupposition",
         "0.6140350877192983",
         "0.5854166666666667",
         "0.7583333333333333",
         "0.5694444444444444"
        ],
        [
         "6",
         "possessed_definites_existence",
         "0.9824561403508771",
         "0.9817030260380015",
         "0.9866666666666667",
         "0.9777777777777779"
        ],
        [
         "7",
         "possessed_definites_uniqueness",
         "0.47368421052631576",
         "0.30303030303030304",
         "0.4842767295597484",
         "0.3888888888888889"
        ],
        [
         "8",
         "question_presupposition",
         "0.9122807017543859",
         "0.907747598313636",
         "0.9425287356321839",
         "0.8925925925925925"
        ],
        [
         "9",
         "total",
         "0.7426900584795322",
         "0.7276206336540098",
         "0.8537618723459431",
         "0.702366255144033"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.949290</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.940741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>both_presupposition</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.983673</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.981481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>change_of_state</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>0.469504</td>\n",
       "      <td>0.697991</td>\n",
       "      <td>0.492593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cleft_existence</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cleft_uniqueness</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.344697</td>\n",
       "      <td>0.817610</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>only_presupposition</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.585417</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.569444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>possessed_definites_existence</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.981703</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.977778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>possessed_definites_uniqueness</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.484277</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.907748</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>0.892593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>total</td>\n",
       "      <td>0.742690</td>\n",
       "      <td>0.727621</td>\n",
       "      <td>0.853762</td>\n",
       "      <td>0.702366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Section  Accuracy  Macro F1  Macro Precision  \\\n",
       "0            all_n_presupposition  0.947368  0.949290         0.962963   \n",
       "1             both_presupposition  0.982456  0.983673         0.986667   \n",
       "2                 change_of_state  0.561404  0.469504         0.697991   \n",
       "3                 cleft_existence  0.719298  0.683333         0.866667   \n",
       "4                cleft_uniqueness  0.491228  0.344697         0.817610   \n",
       "5             only_presupposition  0.614035  0.585417         0.758333   \n",
       "6   possessed_definites_existence  0.982456  0.981703         0.986667   \n",
       "7  possessed_definites_uniqueness  0.473684  0.303030         0.484277   \n",
       "8         question_presupposition  0.912281  0.907748         0.942529   \n",
       "9                           total  0.742690  0.727621         0.853762   \n",
       "\n",
       "   Macro Recall  \n",
       "0      0.940741  \n",
       "1      0.981481  \n",
       "2      0.492593  \n",
       "3      0.666667  \n",
       "4      0.411111  \n",
       "5      0.569444  \n",
       "6      0.977778  \n",
       "7      0.388889  \n",
       "8      0.892593  \n",
       "9      0.702366  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vanilla\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e00a3e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Section",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Macro F1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Macro Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Macro Recall",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "faf3cf0a-389a-4d0c-9f51-7a2a1f2daf0a",
       "rows": [
        [
         "0",
         "all_n_presupposition",
         "0.9298245614035088",
         "0.9319394928193043",
         "0.9523809523809524",
         "0.9203703703703704"
        ],
        [
         "1",
         "both_presupposition",
         "0.9649122807017544",
         "0.9670588235294119",
         "0.9743589743589745",
         "0.9629629629629629"
        ],
        [
         "2",
         "change_of_state",
         "0.5614035087719298",
         "0.46510206671172466",
         "0.690359286103967",
         "0.49074074074074076"
        ],
        [
         "3",
         "cleft_existence",
         "0.7105263157894737",
         "0.6685748360166964",
         "0.8641975308641975",
         "0.6555555555555556"
        ],
        [
         "4",
         "cleft_uniqueness",
         "0.5",
         "0.35934412524699805",
         "0.8190476190476191",
         "0.42037037037037034"
        ],
        [
         "5",
         "only_presupposition",
         "0.6228070175438597",
         "0.5922787193973634",
         "0.7711576310610128",
         "0.5763888888888888"
        ],
        [
         "6",
         "possessed_definites_existence",
         "0.9736842105263158",
         "0.9723551302498671",
         "0.9803921568627452",
         "0.9666666666666667"
        ],
        [
         "7",
         "possessed_definites_uniqueness",
         "0.47368421052631576",
         "0.30168970814132107",
         "0.48286604361370716",
         "0.3888888888888889"
        ],
        [
         "8",
         "question_presupposition",
         "0.9298245614035088",
         "0.9256389814455188",
         "0.9523809523809524",
         "0.912962962962963"
        ],
        [
         "9",
         "total",
         "0.7407407407407407",
         "0.72479578828474",
         "0.8564359224850775",
         "0.6994341563786008"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.931939</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.920370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>both_presupposition</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.967059</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>change_of_state</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>0.465102</td>\n",
       "      <td>0.690359</td>\n",
       "      <td>0.490741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cleft_existence</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.668575</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.655556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cleft_uniqueness</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.359344</td>\n",
       "      <td>0.819048</td>\n",
       "      <td>0.420370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>only_presupposition</td>\n",
       "      <td>0.622807</td>\n",
       "      <td>0.592279</td>\n",
       "      <td>0.771158</td>\n",
       "      <td>0.576389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>possessed_definites_existence</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.972355</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>possessed_definites_uniqueness</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.301690</td>\n",
       "      <td>0.482866</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.925639</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.912963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>total</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.724796</td>\n",
       "      <td>0.856436</td>\n",
       "      <td>0.699434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Section  Accuracy  Macro F1  Macro Precision  \\\n",
       "0            all_n_presupposition  0.929825  0.931939         0.952381   \n",
       "1             both_presupposition  0.964912  0.967059         0.974359   \n",
       "2                 change_of_state  0.561404  0.465102         0.690359   \n",
       "3                 cleft_existence  0.710526  0.668575         0.864198   \n",
       "4                cleft_uniqueness  0.500000  0.359344         0.819048   \n",
       "5             only_presupposition  0.622807  0.592279         0.771158   \n",
       "6   possessed_definites_existence  0.973684  0.972355         0.980392   \n",
       "7  possessed_definites_uniqueness  0.473684  0.301690         0.482866   \n",
       "8         question_presupposition  0.929825  0.925639         0.952381   \n",
       "9                           total  0.740741  0.724796         0.856436   \n",
       "\n",
       "   Macro Recall  \n",
       "0      0.920370  \n",
       "1      0.962963  \n",
       "2      0.490741  \n",
       "3      0.655556  \n",
       "4      0.420370  \n",
       "5      0.576389  \n",
       "6      0.966667  \n",
       "7      0.388889  \n",
       "8      0.912963  \n",
       "9      0.699434  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CoT\n",
    "dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6403b114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Section",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Macro F1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Macro Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Macro Recall",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "81ab7e96-41c4-401b-b455-47a02ddd631c",
       "rows": [
        [
         "0",
         "all_n_presupposition",
         "0.9532163742690059",
         "0.9547806577038512",
         "0.9666666666666667",
         "0.9469135802469136"
        ],
        [
         "1",
         "both_presupposition",
         "0.9590643274853801",
         "0.9614451511376303",
         "0.9704641350210971",
         "0.95679012345679"
        ],
        [
         "2",
         "change_of_state",
         "0.5730994152046783",
         "0.4859044627066578",
         "0.7059952038369305",
         "0.5049382716049383"
        ],
        [
         "3",
         "cleft_existence",
         "0.7368421052631579",
         "0.7111111111111111",
         "0.8717948717948718",
         "0.6888888888888888"
        ],
        [
         "4",
         "cleft_uniqueness",
         "0.5087719298245614",
         "0.38761951796829464",
         "0.7647907647907649",
         "0.433641975308642"
        ],
        [
         "5",
         "only_presupposition",
         "0.6198830409356725",
         "0.5908840598127675",
         "0.7699017615176151",
         "0.5737654320987654"
        ],
        [
         "6",
         "possessed_definites_existence",
         "0.9766081871345029",
         "0.9763814733199201",
         "0.9824561403508771",
         "0.971604938271605"
        ],
        [
         "7",
         "possessed_definites_uniqueness",
         "0.5614035087719298",
         "0.47932664193341196",
         "0.8054092026694767",
         "0.4962962962962963"
        ],
        [
         "8",
         "question_presupposition",
         "0.9298245614035088",
         "0.9248302074517619",
         "0.9523809523809524",
         "0.9123456790123456"
        ],
        [
         "9",
         "total",
         "0.7576348278102664",
         "0.746890729679086",
         "0.8624679529265732",
         "0.7205761316872428"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_n_presupposition</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.954781</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.946914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>both_presupposition</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.961445</td>\n",
       "      <td>0.970464</td>\n",
       "      <td>0.956790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>change_of_state</td>\n",
       "      <td>0.573099</td>\n",
       "      <td>0.485904</td>\n",
       "      <td>0.705995</td>\n",
       "      <td>0.504938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cleft_existence</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.688889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cleft_uniqueness</td>\n",
       "      <td>0.508772</td>\n",
       "      <td>0.387620</td>\n",
       "      <td>0.764791</td>\n",
       "      <td>0.433642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>only_presupposition</td>\n",
       "      <td>0.619883</td>\n",
       "      <td>0.590884</td>\n",
       "      <td>0.769902</td>\n",
       "      <td>0.573765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>possessed_definites_existence</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>0.976381</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.971605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>possessed_definites_uniqueness</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>0.479327</td>\n",
       "      <td>0.805409</td>\n",
       "      <td>0.496296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>question_presupposition</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.924830</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.912346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>total</td>\n",
       "      <td>0.757635</td>\n",
       "      <td>0.746891</td>\n",
       "      <td>0.862468</td>\n",
       "      <td>0.720576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Section  Accuracy  Macro F1  Macro Precision  \\\n",
       "0            all_n_presupposition  0.953216  0.954781         0.966667   \n",
       "1             both_presupposition  0.959064  0.961445         0.970464   \n",
       "2                 change_of_state  0.573099  0.485904         0.705995   \n",
       "3                 cleft_existence  0.736842  0.711111         0.871795   \n",
       "4                cleft_uniqueness  0.508772  0.387620         0.764791   \n",
       "5             only_presupposition  0.619883  0.590884         0.769902   \n",
       "6   possessed_definites_existence  0.976608  0.976381         0.982456   \n",
       "7  possessed_definites_uniqueness  0.561404  0.479327         0.805409   \n",
       "8         question_presupposition  0.929825  0.924830         0.952381   \n",
       "9                           total  0.757635  0.746891         0.862468   \n",
       "\n",
       "   Macro Recall  \n",
       "0      0.946914  \n",
       "1      0.956790  \n",
       "2      0.504938  \n",
       "3      0.688889  \n",
       "4      0.433642  \n",
       "5      0.573765  \n",
       "6      0.971605  \n",
       "7      0.496296  \n",
       "8      0.912346  \n",
       "9      0.720576  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Few-shot\n",
    "dfs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e374164",
   "metadata": {},
   "source": [
    "### Comparison to Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45e9880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_mistakes(results):\n",
    "    mistakes = []\n",
    "    for res in results:\n",
    "        if res['pred_label'] != res['gold_label']:\n",
    "            mistakes.append(res)\n",
    "    return mistakes\n",
    "\n",
    "mistakes_vanilla = model_mistakes(results_vanilla)\n",
    "mistakes_cot = model_mistakes(results_cot)\n",
    "mistakes_fewshot = model_mistakes(results_fewshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7a7ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('mistakes_vanilla.json', 'w') as f:\n",
    "    json.dump(mistakes_vanilla, f, indent=2)\n",
    "with open('mistakes_cot.json', 'w') as f:\n",
    "    json.dump(mistakes_cot, f, indent=2)\n",
    "with open('mistakes_fewshot.json', 'w') as f:\n",
    "    json.dump(mistakes_fewshot, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d348615",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imppres_deberta_mistakes.json', 'r') as f:\n",
    "    mistakes_deberta = json.load(f)\n",
    "narrowed_test_lookup = {(m['premise'], m['hypothesis'], m['section']) for m in narrowed_test}\n",
    "narrowed_mistakes_deberta = [m for m in mistakes_deberta if (m['premise'], m['hypothesis'], m['section']) in narrowed_test_lookup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24b93c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement matrix for vanilla vs DeBERTa:\n",
      "                  vanilla Correct  vanilla Mistakes\n",
      "DeBERTa Correct               238                58\n",
      "DeBERTa Mistakes              143                74\n",
      "\n",
      "Agreement matrix for cot vs DeBERTa:\n",
      "                  cot Correct  cot Mistakes\n",
      "DeBERTa Correct           235            61\n",
      "DeBERTa Mistakes          144            73\n",
      "\n",
      "Agreement matrix for fewshot vs DeBERTa:\n",
      "                  fewshot Correct  fewshot Mistakes\n",
      "DeBERTa Correct               243                53\n",
      "DeBERTa Mistakes              163                54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_lookup(mistakes):\n",
    "    \"\"\"Create a lookup for mistakes based on premise, hypothesis, and section.\"\"\"\n",
    "    return {(m['premise'], m['hypothesis'], m['section']) for m in mistakes}\n",
    "\n",
    "mistakes_vanilla_lookup = get_lookup(mistakes_vanilla)\n",
    "mistakes_cot_lookup = get_lookup(mistakes_cot)\n",
    "mistakes_fewshot_lookup = get_lookup(mistakes_fewshot)\n",
    "mistakes_deberta_lookup = get_lookup(narrowed_mistakes_deberta)\n",
    "\n",
    "for program in ['vanilla', 'cot', 'fewshot']:\n",
    "    mistakes_prog_lookup = globals()[f'mistakes_{program}_lookup']\n",
    "    agreement_matrix = [[0, 0], [0, 0]]\n",
    "    for item in narrowed_test:\n",
    "        key = (item['premise'], item['hypothesis'], item['section'])\n",
    "        agreement_matrix[int(key in mistakes_deberta_lookup)][int(key in mistakes_prog_lookup)] += 1\n",
    "    print(f\"Agreement matrix for {program} vs DeBERTa:\")\n",
    "    print(pd.DataFrame(agreement_matrix, \n",
    "          index=['DeBERTa Correct', 'DeBERTa Mistakes'], \n",
    "          columns=[f'{program} Correct', f'{program} Mistakes']))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2 (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
